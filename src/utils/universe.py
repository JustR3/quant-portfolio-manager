"""
Universe Selection with Survivorship Bias Control

Provides hybrid approach (Option C):
- Current S&P 500 constituents
- Plus major delistings from 2000-2025
- ~95% accuracy for alpha research

For production/publishing, upgrade to full historical constituent tracking.
"""

import pandas as pd
from pathlib import Path
from typing import List, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


def _get_static_sp500_list() -> List[str]:
    """
    Static S&P 500 list (Dec 2024) - used when Wikipedia scraping fails.
    Contains ~500 current constituents.
    """
    return [
        'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'NVDA', 'META', 'TSLA', 'BRK-B', 'UNH',
        'XOM', 'LLY', 'JPM', 'V', 'JNJ', 'AVGO', 'PG', 'MA', 'HD', 'CVX',
        'MRK', 'ABBV', 'COST', 'PEP', 'KO', 'ADBE', 'WMT', 'MCD', 'CSCO', 'CRM',
        'ACN', 'LIN', 'TMO', 'ABT', 'NFLX', 'AMD', 'BAC', 'ORCL', 'DIS', 'VZ',
        'WFC', 'PM', 'TXN', 'INTU', 'CMCSA', 'AMGN', 'QCOM', 'IBM', 'CAT', 'RTX',
        'HON', 'GE', 'AMAT', 'UPS', 'LOW', 'ISRG', 'SPGI', 'MS', 'NEE', 'PFE',
        'T', 'DHR', 'BLK', 'AXP', 'COP', 'BMY', 'DE', 'SYK', 'BA', 'PLD',
        'BKNG', 'MDT', 'VRTX', 'TJX', 'GS', 'LMT', 'GILD', 'ADI', 'MDLZ', 'AMT',
        'C', 'SBUX', 'ADP', 'MMC', 'TMUS', 'CB', 'SO', 'REGN', 'CI', 'BDX',
        'SCHW', 'CVS', 'ETN', 'EOG', 'MO', 'ZTS', 'DUK', 'PGR', 'NOC', 'FI',
        'USB', 'ITW', 'MU', 'BSX', 'SLB', 'EQIX', 'TGT', 'AON', 'HCA', 'EW',
        'PNC', 'LRCX', 'MMM', 'ICE', 'CL', 'WM', 'MCO', 'APD', 'CME', 'NSC',
        'KLAC', 'FCX', 'MAR', 'SHW', 'PYPL', 'ATVI', 'CSX', 'APH', 'GD', 'GM',
        'SNPS', 'ECL', 'ADSK', 'TT', 'EL', 'MSI', 'PSA', 'CDNS', 'AEP', 'HUM',
        'MCK', 'TDG', 'ROP', 'AFL', 'PH', 'NXPI', 'AIG', 'FDX', 'O', 'AJG',
        'F', 'PCAR', 'CCI', 'SRE', 'NEM', 'EMR', 'OXY', 'TFC', 'JCI', 'MET',
        'ORLY', 'TEL', 'FTNT', 'CHTR', 'PAYX', 'AZO', 'WELL', 'D', 'ANET', 'ROST',
        'ALL', 'PCG', 'SPG', 'TRV', 'IQV', 'MSCI', 'CARR', 'KMB', 'DHI', 'DLR',
        'AME', 'KMI', 'BIIB', 'VLO', 'COF', 'SYY', 'MNST', 'PSX', 'CTAS', 'PRU',
        'CMG', 'EXC', 'YUM', 'RSG', 'BK', 'HLT', 'BKR', 'A', 'HSY', 'IDXX',
        'AMP', 'DD', 'EA', 'GWW', 'CTVA', 'KDP', 'DXCM', 'PPG', 'FAST', 'VRSK',
        'EXR', 'XEL', 'ED', 'VICI', 'GLW', 'KHC', 'ROK', 'GEHC', 'ODFL', 'RMD',
        'CPRT', 'WMB', 'OTIS', 'NUE', 'CTSH', 'CNC', 'DOW', 'GIS', 'FANG', 'IT',
        'VMC', 'AWK', 'DVN', 'HPQ', 'STZ', 'CBRE', 'DAL', 'WBD', 'SBAC', 'MTD',
        'ANSS', 'LHX', 'IFF', 'MLM', 'HAL', 'WEC', 'HIG', 'KEYS', 'HES', 'ZBH',
        'PWR', 'EBAY', 'ETR', 'IR', 'PPL', 'HOLX', 'FITB', 'WTW', 'IRM', 'TSN',
        'UAL', 'MTB', 'DLTR', 'DFS', 'AVB', 'CAH', 'DTE', 'STT', 'LYB', 'RJF',
        'LH', 'HPE', 'URI', 'EIX', 'ACGL', 'DG', 'EFX', 'HBAN', 'WAB', 'FTV',
        'TROW', 'CSGP', 'TTWO', 'CCL', 'SWKS', 'BAX', 'PEG', 'INVH', 'APTV', 'NTRS',
        'AEE', 'WAT', 'ALGN', 'ARE', 'NDAQ', 'TDY', 'VTR', 'BALL', 'CNP', 'OMC',
        'FE', 'RF', 'K', 'STE', 'ZBRA', 'EXPE', 'ATO', 'FDS', 'NTAP', 'POOL',
        'LYV', 'CFG', 'TSCO', 'LUV', 'CLX', 'MPWR', 'ES', 'AVY', 'DOV', 'WY',
        'BLDR', 'MAA', 'KEY', 'SYF', 'TYL', 'CAG', 'IEX', 'WST', 'DRI', 'CE',
        'VLTO', 'EQR', 'MOH', 'HUBB', 'ESS', 'LDOS', 'CBOE', 'BR', 'ALB', 'ULTA',
        'GRMN', 'GPN', 'IP', 'STX', 'SWK', 'TER', 'AKAM', 'LVS', 'JBHT', 'PKI',
        'UDR', 'PTC', 'LKQ', 'CINF', 'NVR', 'J', 'DPZ', 'EPAM', 'CMS', 'BG',
        'COO', 'DGX', 'TECH', 'ROL', 'NRG', 'BBY', 'PAYC', 'JKHY', 'TXT', 'EXPD',
        'CPT', 'EVRG', 'MKC', 'TRMB', 'VRSN', 'CHRW', 'CF', 'AOS', 'INCY', 'BBWI',
        'NDSN', 'HST', 'AMCR', 'MAS', 'FMC', 'PEAK', 'GL', 'CPB', 'BXP', 'L',
        'AAL', 'BEN', 'IPG', 'RL', 'TAP', 'HII', 'MKTX', 'AIZ', 'SEE', 'PNR',
        'REG', 'IVZ', 'UHS', 'BWA', 'MTCH', 'FFIV', 'WYNN', 'MGM', 'XRAY', 'NWSA',
        'NWS', 'HAS', 'PARA', 'DISH', 'NI', 'APA', 'DVA', 'AAP', 'WHR', 'ZION',
        'CRL', 'LNC', 'FRT', 'ALK', 'GNRC', 'PHM', 'JNPR', 'TPR', 'KIM', 'MHK',
        'EMN', 'FOX', 'FOXA', 'PNW', 'VFC', 'HWM'
    ]


def get_sp500_current() -> List[str]:
    """
    Fetch current S&P 500 constituents from Wikipedia or fallback to static list.
    
    Returns:
        List of ticker symbols
    """
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        
        # Add headers to avoid 403 Forbidden
        import urllib.request
        req = urllib.request.Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        with urllib.request.urlopen(req) as response:
            tables = pd.read_html(response)
            df = tables[0]
            tickers = df['Symbol'].tolist()
            
            # Clean tickers (replace dots with dashes for Yahoo Finance)
            tickers = [t.replace('.', '-') for t in tickers]
            
            logger.info(f"‚úÖ Fetched {len(tickers)} current S&P 500 tickers from Wikipedia")
            return tickers
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to fetch from Wikipedia: {e}")
        logger.info("Using static S&P 500 list...")
        
        # Fallback to static list (top ~100 by market cap as of Dec 2024)
        return _get_static_sp500_list()


def get_major_delistings() -> List[str]:
    """
    Get major delistings to add to universe (survivorship bias control).
    
    Returns:
        List of delisted ticker symbols
    """
    delistings_file = Path("data/historical/metadata/major_delistings.csv")
    
    if not delistings_file.exists():
        logger.warning("‚ö†Ô∏è  Major delistings file not found, returning empty list")
        return []
    
    try:
        df = pd.read_csv(delistings_file)
        
        # Filter to tickers that were actually delisted (not still trading)
        df_delisted = df[df['delisting_date'] != 'Never']
        
        tickers = df_delisted['ticker'].tolist()
        logger.info(f"‚úÖ Added {len(tickers)} major delistings for survivorship bias control")
        
        return tickers
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load delistings: {e}")
        return []


def get_hybrid_universe() -> List[str]:
    """
    Get hybrid universe: Current S&P 500 + major delistings.
    
    This is Option C (pragmatic approach):
    - ~95% accuracy for alpha research
    - Current S&P 500: ~500 tickers
    - Major delistings: ~10-20 tickers (2008 crisis, etc.)
    
    Returns:
        List of unique ticker symbols
    """
    # Get current S&P 500
    current = get_sp500_current()
    
    # Add major delistings
    delistings = get_major_delistings()
    
    # Combine and deduplicate
    universe = list(set(current + delistings))
    universe.sort()
    
    logger.info(f"üìä Hybrid universe: {len(current)} current + {len(delistings)} delistings = {len(universe)} total")
    
    return universe


def get_universe(
    as_of_date: Optional[datetime] = None,
    include_delistings: bool = True
) -> List[str]:
    """
    Get investment universe with survivorship bias control.
    
    Args:
        as_of_date: Date for point-in-time universe (currently ignored in hybrid approach)
        include_delistings: Whether to include major delistings (default: True)
        
    Returns:
        List of ticker symbols
        
    Notes:
        - Current implementation uses hybrid approach (Option C)
        - as_of_date parameter reserved for future upgrade to full historical tracking
        - For production, consider upgrading to full historical constituent data
    """
    if as_of_date is not None:
        logger.warning(
            "‚ö†Ô∏è  as_of_date parameter not yet implemented in hybrid approach. "
            "Returning current universe + delistings. "
            "For point-in-time accuracy, upgrade to full historical constituent tracking."
        )
    
    if include_delistings:
        return get_hybrid_universe()
    else:
        return get_sp500_current()


def save_universe_snapshot(output_file: Optional[Path] = None):
    """
    Save current universe to file for reference.
    
    Args:
        output_file: Output path (default: data/historical/metadata/universe_snapshot_{date}.csv)
    """
    universe = get_hybrid_universe()
    
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d")
        output_file = Path(f"data/historical/metadata/universe_snapshot_{timestamp}.csv")
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # Create DataFrame with metadata
    df = pd.DataFrame({
        'ticker': universe,
        'snapshot_date': datetime.now().isoformat(),
        'source': 'hybrid_universe'
    })
    
    df.to_csv(output_file, index=False)
    logger.info(f"üíæ Universe snapshot saved: {output_file}")
    
    return output_file


if __name__ == "__main__":
    # Test the universe functions
    logging.basicConfig(level=logging.INFO)
    
    print("\n" + "="*60)
    print("HYBRID UNIVERSE TEST")
    print("="*60)
    
    # Get hybrid universe
    universe = get_hybrid_universe()
    
    print(f"\nüìä Total universe size: {len(universe)} tickers")
    print(f"\nFirst 10 tickers: {universe[:10]}")
    print(f"Last 10 tickers: {universe[-10:]}")
    
    # Check for delistings
    delistings = get_major_delistings()
    print(f"\n‚ö†Ô∏è  Major delistings included: {delistings}")
    
    # Save snapshot
    output_file = save_universe_snapshot()
    print(f"\n‚úÖ Snapshot saved to: {output_file}")
